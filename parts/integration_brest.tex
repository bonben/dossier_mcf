%!TEX root = ../dossier_candidature_mcf_brest.tex

\section{Projets d'intégration en recherche et en enseignement}\vspace{1cm}

\subsection{Projet d'intégration en recherche}\vspace{2em}

La thématique de recherche proposée pour le poste à pourvoir est celle de l'adéquation entre algorithmes et architectures dans le domaine de l'électronique numérique. Les applications dans le domaine de l'intelligence artificielle (IA) embarquée seront priviligiées. Le domaine de l'IA, en particulier celui de l'apprentissage machine, est actuellement en plein essor. Citons quelques exemples pour l'illustrer : en médecine, des algorithmes d'apprentissage machine parviennent désormais à établir, en ce qui concerne certaines pathologies, de meilleurs diagnostics qu'un panel d'experts \cite{esteva2017dermatologist,hannun2019cardiologist}. Dans le domaine de l'automobile et plus généralement des véhicules autonomes \cite{wang2018enhancing}, les progrès sont fulgurants. Ils le sont également en ce qui concerne la reconnaissance d'images et l'analyse de flux vidéos comme le montrent par exemples leurs résultats dans le challenge ImageNet \cite{ILSVRC15}, la traduction linguistique avec des applications comme DeepL\footnote{\url{https://www.deepl.com}} ou encore les jeux de société avec les réussites d'AlphaGo \cite{silver2016mastering}. L'intelligence artificielle parvient même à égaler l'humain dans des domaines qui lui semblent pourtant propre comme l'art (DeepArt\footnote{\url{https://deepart.io}}) ou certaines compétences sociales : des réseaux de neurones parviennent à reconnaitre des émotions de manière plus fiable que des humains \cite{janssen2013machines}.

L'avènement de l'IA aura des conséquences majeures et il est nécessaire pour les états comme pour les organismes de recherche de prendre part à cette transformation pour préserver leur indépendance. Cette idée est bien expliquée dans ce paragraphe extrait du rapport de Cédric Villani sur l'IA \cite{villani2018donner} :

\begin{quote}
	\og Mais la spécificité la plus importante de l'IA par rapport aux autres domaines scientifiques est son impact sur l'ensemble de la société, qui, loin d'être une mode passagère ou un simple phénomène médiatique, promet d'avoir des conséquences primordiales et durables au niveau mondial. L'IA aujourd'hui irrigue tous les domaines, économiques, sociétaux, politiques, culturels... Et la plupart des grandes puissances économiques, qu'elles soient nationales ou privées, l'ont bien compris et investissent massivement dans l'IA. L'enjeu n'est rien moins que le choix de la société dans laquelle nous voulons vivre demain. Et nous devons préserver notre indépendance en la matière si nous ne voulons pas voir ces choix imposés par d'autres. Or parmi les rares atouts de la France en la matière, il y a l'excellence de notre formation scientifique, et des cerveaux qui en sont issus. Il convient de tout faire pour la préserver, la renforcer, et la transformer en succès scientifiques et économiques dans le respect de nos valeurs.\fg
\end{quote}

Du point de vue d'un chercheur en électronique numérique, je vois au moins trois points sur lesquels agir pour développer cette indépendance :

\begin{itemize}
	\item Indépendance vis-à-vis de la quantité de calculs et des capacités de mémoires : un nombre très réduit d'acteurs détient aujourd'hui la plus grande partie de la puissance de calcul mondiale. Cette puissance de calcul leur donne fatalement une avance considérable sur leurs concurrents, mais aussi sur les états. Quel impact cet avantage peut-il avoir en terme de souveraineté des états ? Quelle place pourront occuper les entreprises concurrentes de tailles plus modeste ? Pour mitiger ce déséquilibre, une des voies possibles est la conception d'architectures hétérogènes et d'outils puissant pour les utiliser (Section \ref{subsubsec:archis_heterogenes}).
	\item Indépendance vis-à-vis du matériel embarqué : les industriels doivent pouvoir concevoir leurs propres circuits numériques embarqués, pour s'assurer de la sécurité des données. des méthodologies efficaces de conception électronique peuvent contribuer à cette indépendance comme les méthodologies ASIP développées dans la Section \ref{subsubsec:asip_ia} de ce projet de recherche.
	\item Indépendance vis-à-vis des méthodes : les entreprises doivent prendre par à la révolution de l'IA en s'appropriant ses méthodes. Cet apprentissage peut passer par des échanges entre chercheurs et industriels. Dans ce cadre, j'ai rédigé en partenariat avec l'entreprise WorldCast Systems un développement de projet CIFRE qui pourra servir de base à une future postulation pour un financement de l'ANRT. Ce projet porte sur l'utilisation d'algorithme d'apprentissage machine appliquée à la maintenance prédictive (Section \ref{subsubsec:cifre}).
\end{itemize}

Enfin je propose dans une dernière partie un projet qui fait le lien entre mes travaux de thèse et d'ATER et la thématique de l'intelligence articielle. Il s'agit d'utiliser un logiciel de simulation de chaînes de communications pour entraîner un constructeur de codes correcteurs d'erreurs. Ce thème est décrit en Section \ref{subsubsec:ia_fec}.

% Ajouter des citations

\subsubsection{Architectures hétérogènes}
\label{subsubsec:archis_heterogenes}
Aujourd'hui, les phases d'apprentissage des systèmes d'intelligence artificielles sont réalisées à l'aide d'unités de calculs graphiques (GPU : Graphical Processing Units). L'inconvénient des GPU est leur forte consommation énergétique. Cette consommation pose deux problèmes : celui des coûts financiers et celui de l'impact écologique \cite{DBLP:journals/corr/abs-1906-02243}. Des alternatives sont d'ores et déjà proposées, avec par exemple le développement de puces TPU (Tensor Processing Unit) développées par l'entreprise Google, qui permettent un apprentissage plus rapide et une consommation réduite \cite{8192463}. Le troisième type de cibles utilisées est le FPGA (Field Programmable Gate Array), dont le principal avantage est qu'il peut être reconfiguré pour s'adapter finement à l'algorithme que l'on souhaite lui faire exécuter. Cette spécialisation peut permettre des réductions importantes de consommation énergétique \cite{7505926}. Ces caractéristiques mènent à la tendance actuelle de généralisation de la présence des FPGA dans les centre de données. Cependant, pour rendre cette technologie accessible et utilisée efficacement, des couches d'abstraction sont nécessaires\footnote{\url{https://systemdesign.intel.com/fpgas-data-centers-takes-stack/}} :

\begin{itemize}
	\item Bibliothèques logicielles pour calcul parallèle (C++ STL\footnote{\url{https://github.com/intel/parallelstl}}, SyCL \cite{keryell2015khronos})
	\item Langages et compilateurs pour cibles parallèles (OpenCL \cite{5457293}, TCE \cite{TCEToolset})
	\item Langages (SystemC, System Verilog) et outils de synthèse matérielle haut niveau (Vivado HLS, Intel HLS Compiler)
\end{itemize}

Le développement de ces outils et leur applications sur les futurs algorithmes d'apprentissage permettront des améliorations majeures de l'efficacité énergétique et de la vitesse d'apprentissage. Mes parcours professionnel et universitaire au cours desquels j'ai développé une double compétence logicielle et matérielle me permettront de contribuer significativement au domaine par des contributions au développement de ces outils ou à leur utilisation.

\subsubsection{ASIP pour l'IA}
\label{subsubsec:asip_ia}

La mise en \oe{}uvre de l'intelligence artificielle dans les produits industriels pose un problème de confidentialité des données. De plus, pour atteindre des performances satisfaisantes, du point de vue du temps d'exécution ou de l'efficacité énergétique, il sera nécessaire d'utiliser des accélérateurs matériels. Ces accélérateurs peuvent prendre par exemple la forme de processeurs spécialisés (Application Specific Instruction set Processors). Ceux-ci permettent d'atteindre des performances supérieures aux processeurs généralistes tout en conservant un haut degré de flexibilité. Cette flexibilité peut s'avérer bénéfique pour l'implémentation de réseaux de neurones convolutionnels \cite{IJzerman:2018:AEE:3229631.3229637}.

Les architectures TTA sont un modèle de processeurs spécialisés. Ils bénéficient d'un environnement de conception nommé TCE qui facilite leur conception. Le modèle de processeur, configuré par le concepteur, est fourni en langage VHDL générique. Cela permet d'avoir le contrôle complet de l'architecture matérielle. Utiliser cet outil évite donc toute dépendance à un composant matériel soumis à une propriété industrielle.

\includepdf[pages=1,pagecommand={\subsubsection{Développement d'un projet de thèse CIFRE}\label{subsubsec:cifre}},width=\paperwidth, offset=80 -100]{pieces/cifre.pdf}
\includepdf[pages=2-,pagecommand={},width=\paperwidth, offset=80 -80]{pieces/cifre.pdf}


\subsubsection{Algorithmes d'apprentissage pour la conception de codes correcteurs d'erreurs}
\label{subsubsec:ia_fec}

L'art de de la construction des codes correcteurs d'erreurs est un domaine de recherche actif depuis la proposition de la théorie de l'information de Claude Shannon en 1948 []. Diverses familles ont été proposées, de plus en plus complexes. Sur le plan conceptuel, les outils mathématiques et d'analyse sont de plus en plus raffinés. Sur le plan calculatoire, les traitements à réaliser au niveau du décodage sont de plus en plus lourds. Cette complexité du décodage a suivi la croissance exponentielle de la puissance de calcul des circuits électroniques (loi de Moore).

Aujourd'hui, la conception de ces codes correcteurs d'erreurs et des algorithmes de décodage qui leur sont associés est toujours le fruit du travail d'une communauté d'experts. Ils utilisent pour cela des outils mathématiques théoriques couplés à des expérimentations empiriques.

La question désormais est de savoir si les algorithmes d'apprentissage machine peuvent être utilisée pour la conception et le décodage de codes correcteurs d'erreurs. Certains travaux présenent d'ores et déja des résultats intéressants, en utilisant des réseaux de neurones profonds pour l'encodage et le décodage \cite{7926071,NIPS2018_8154}. Une autre approche est possible, présentée dans \cite{DBLP:journals/corr/abs-1901-05719}.

\begin{figure}[t]
	\centering
	\includegraphics{pieces/schema_ai_coding.pdf}
	\caption{Apprentissage machine pour la construction de codes de canal.}
	\label{fig:ai_coding}
\end{figure}

La première étape est de partir d'une famille de code existante. Pour l'exemple, et parce qu'il s'agit des codes que je connais le mieux, je parlerai la famille des codes polaires. Des raisonnements analogues pourraient être menés pour n'importe quelle famille de codes correcteurs d'erreurs.
La construction d'un code polaire $(N,K)$, d'une taille $N$ et codant $K$ bits d'informations, peut être représentée par son vecteur de bits gelés $A_k$, dont $K$ bits doivent être à 1 et les $N-K$ restants à 0. Cette construction est la donnée de sortie du \og constructeur \fg de la Figure \ref{fig:ai_coding}. Elle est possible de générer un encodeur et un décodeur de codes polaires, et de simuler ses performances, par exemple à l'aide d'une simulation de type Monte Carlo. Le résultat de cette évaluation, nommé \og mesure de performance \fg dans la figure, est le taux d'erreur trame simulé.

Le principe est alors de réaliser, dans le \og constructeur \fg, un système d'apprentissage automatique qui doit améliorer les performances du code. Une telle démarche est décrite dans \cite{DBLP:journals/corr/abs-1904-07511}.

Ce type d'approche me parait très prometteur. Il s'agit en effet de prendre un problème ayant un très grand nombre de dimensions (N, K, position des bits gelés, algorithme de décodage utilisé, ...) et de réduire ce problème à l'aide d'outil d'apprentissage machine (SVM, DNN) qui sont très performants dans ce type de traitement. Il peut y avoir des applications directes, comme dans \cite{DBLP:journals/corr/abs-1904-07511} où l'apprentissage par renforcement permet des gains substantiels en termes de performances de décodage. Il peut également y avoir des approches plus complexes, et en particulier un aller retour entre les experts, qui proposent un code, et la réponse du système d'apprentissage, qui proposera peut être des axes d'améliorations inattendus. Des allers-retours entre l'humain et la machine pourraient alors mener vers la définition de nouvelles familles de codes correcteurs d'erreurs.

Mes travaux antérieurs seront un tremplin parfait pour effectuer cette recherche. L'évaluateur de la Figure \ref{fig:ai_coding} est la partie critique du mécanisme en termes de temps de calcul. En effet, à chaque itération, une simulation Monte Carlo doit être effectuée. Or pour qu'une mesure soit fiable, un nombre raisonnable d'erreurs doit être simulé (typiquement 100). Pour des taux d'erreur trame faibles, le temps de simulation devient extrèmement long. Or le projet AFF3CT\footnote{\url{https://aff3ct.github.io}} auquel je participe très activement permet justement la simulation très haut débit de l'ensemble des différents codes correcteurs d'erreurs, et ce de manière très générique et flexible vis-à-vis des paramètres d'entrée. Cette généricité permettra, dans le mécanisme d'apprentissage représenté en Figure \ref{fig:ai_coding}, d'explorer toutes les dimensions du problèmes. En continuant sur l'exemple des codes polaires, il sera par exemple possible d'explorer tout couple (N,K), tout vecteur de bit gelés, tout motif de poinçonnage, et la majeure partie des algorithmes de décodage existant. Pour chaque algorithme de décodage, il est également possible de modifier des paramètres internes. Pour le décodage SCL (Successive Cancellation List) ces paramètres sont par exemple la profondeur de la liste, l'élagage de l'arbre de décodage, la quantification des données, le polynome d'un éventuel CRC concaténé.

De plus, ce type d'apprentissage peut s'appliquer à d'autres éléments de la chaine de communication, comme le codage source, la génération des formes d'ondes ou les étapes de synchronisation.

\renewcommand{\refname}{Références}
\newrefcontext[sorting=none]
\printbibliography[resetnumbers=true,keyword={projet_recherche}]

\newpage
\vspace{2em}
\subsection{Perspectives de collaborations internationales}
Il est attendu du candidat de pouvoir justifier d'une expérience à l'étranger.
J'ai effectué mon doctorat en cotutelle entre l'Université de Bordeaux, en France, et l'\'Ecole Polytechnique de Montréal, au Canada.
J'ai également collaboré avec des chercheurs de l'Université de Tampere pour une de mes contributions.
Dans cette section, je donne quelques perspectives de collaborations internationales.

\subsubsection{\'Ecole Polytechnique de Montréal, Canada}
Ayant travaillé pendant 18 mois à Polytechnique Montréal, j'ai eu l'occasion de créer de nombreux liens avec des professeurs et étudiants. Parmi eux, le Pr. Yvon Savaria fut mon directeur de thèse. Sa lettre de recommandation (section \ref{subsec:yvon}) est jointe. Notre collaboration ayant été agréable et fructueuse par le passé, nous souhaitons continuer à travailler ensemble dans le futur. Yvon Savaria travaille depuis de nombreuses années sur des thématiques du domaine de l'électronique. Les interactions possibles sont nombreuses : thèses en cotutelles, financement de projet par des industriels partenaires, stages ingénieur, master.
Le Pr. Pierre Langlois est coauteur d'un de mes articles sur une implémentation ASIP \cite{Leonardon2018a}. Il s'intéresse en effet beaucoup à ce type d'architecture, que ce soit en enseignement ou en recherche. Nous conservons de bons rapports et cette thématique d'implémentation ASIP pourrait être un point de rencontre.

\subsubsection{Université McGill, Canada}
L'équipe du Pr. Warren Gross est un contributeur majeur du domaine des codes correcteurs d'erreur. J'ai des contacts très réguliers avec le Dr. Thibaud Tonnellier qui, comme moi, a effectué sa thèse au sein du laboratoire IMS. Nous sommes tous deux coauteurs d'un article sur le logiciel AFF3CT et d'un autre en cours de soumission. Encore une fois, il sera possible de créer des partenariats sous la forme d'échanges d'étudiants ou bien de thèse en cotutelle.

\subsubsection{Université de Tampere, Finlande}
Mes travaux sur le décodeur de code polaires d'architecture TTA m'a amené à collaborer avec Pekka Jääskeläinen de l'Université de Tampere. Coauteur de l'un de mes articles, nous avons pour ambition de continuer à travailler ensemble. Le paradigme TTA me paraît en effet prometteur et les outils développés dans le projet TCE sont pertinents. Ces outils pourraient être utilisés pour des implémentations matérielles de décodeurs de codes correcteurs d'erreurs ou d'autres fonctionnalités d'une chaîne de communication. 

\newpage
\subsection{Projet d'intégration en enseignement}\vspace{2em}
\subsubsection{Contexte}
Le contexte des activités d'enseignement à l'IMT Atlantique est particulier puisque, suite à la récente fusion, l'ensemble du contenu de formation est revu. Désormais, les étudiants de première année suivront des formations de tronc commun dont le rôle est de bâtir un socle commun. Ce tronc commun sert également à présenter aux élèves les Thématiques d'ApproFondissement (TAF). Chaque étudiant doit choisir une TAF pour chacune des deuxième et troisième années. La TAF est constituée de plusieurs Unités d'Enseignement (UE). 3 soit obligatoires (UE c\oe{}urs). 6 sont choisies par les étudiants parmi une liste propre à la TAF (UE électives). L'équipe pédagogique d'une TAF propose un certain nombre d'UE électives (4 dans le cas de la TAF SEH). Les UE restantes de la liste d'UE électives sont celles d'autres TAF. Les 3 dernières UE peuvent être choisies par l'étudiant parmi n'importe quelle UE proposée par l'IMT Atlantique.
Les enseignants-chercheurs du département \'Electronique sont principalement impliqués dans deux TAF. La première est la TAF Systèmes Embarqués Hétérogènes (SEH). La seconde est nommée Conception d'Objets Communicants (CoOC).

\subsubsection{Enseignements de 1\textsuperscript{ère} année}

Dans les enseignements de première année, je suis opérationnel pour l'enseignement de l'électronique numérique. Mes expériences d'enseignement correspondent en effet aux sujets abordés (cf \ref{subsubsec:loto}, \ref{subsubsec:reconf}, \ref{subsubsec:rsi}, \ref{subsubsec:en1}).
 Le sujet du premier TP est par exemple la conception d'un projet Loto, tel que j'ai pu l'enseigner à l'ENSEIRB-MATMECA (cf. \ref{subsubsec:loto}).

\subsubsection{TAF Systèmes Embarqués Hétérogènes}
L'objectif de cette TAF est de former les étudiants ingénieurs à la conception et à la mise en \oe{}uvre de systèmes embarqués et hétérogènes. Les étudiants doivent développer une double compétence logicielle et matérielle. J'ai eu au cours de mon parcours de nombreuses expériences dans ces deux compétences. Dans la suite de cette section, je vais lister les différentes UE de la TAF SEH et montrer que je peux intervenir dans chacune d'entre elles. Une emphase particulière est mise sur l'UE élective 4, car il existe un besoin particulier dans cette UE après le départ de l'enseignant qui en était responsable. Je montre pourquoi je pense pouvoir la prendre en charge à moyen terme.

\paragraph{UE C\oe{}ur 1 : Circuits intégrés numériques et analogiques}
Le sujet de cette UE est la conception de circuits numériques et analogiques. Le langage VHDL est abordé avec un FPGA comme cible.
J'ai une expérience certaine dans ces domaines du fait des mes enseignements à l'ENSEIRB-MATMECA (cf. \ref{subsubsec:loto}, \ref{subsubsec:reconf}, \ref{subsubsec:rsi}, \ref{subsubsec:en1}).

\paragraph{UE C\oe{}ur 2 : Méthodologies - de l'algorithme à la puce}
Sont abordées dans cette UE des méthodologies de conception de circuits numériques ou analogiques à partir d'une description algorithmiques et de contriantes de performances et de complexité. J'ai suivi à Polytechnique Montréal un cours très proche donné par le Pr. Jean-Pierre David (ELE8307 - \url{http://www.groupes.polymtl.ca/ele8307/}). Dans le cadre de ce cours est par exemple abordée la méthodologie des machines algorithmiques qui permet de passer d'un algorithme décrit en C à une architecture matérielle composée d'une unité de contrôle et d'une unité de calcul. Ces problématiques me sont donc familières et je pourrai apporter une approche nouvelle.

\paragraph{UE C\oe{}ur 3 : Systèmes Embarqués}
J'ai effectué une formation d'ingénieur par apprentissage à l'ENSEIRB-MATMECA, dans la filière Systèmes \'Electroniques Embarqués. Dans le cadre de cette formation j'ai abordé les principaux thèmes de cette UE, la configuration d'un système d'exploitation temps réel, le développement de pilotes matériels, les différents bus de communications. Ensuite et surtout, j'ai développé ces compétences au sein de mon entreprise d'accueil, WorldCast Systems, au sein de laquelle j'ai travaillé sur la programmation de pilotes pour contrôler les différents éléments d'émetteurs pour la radio FM.

\paragraph{UE \'Elective 1 : Conception haut niveau de circuits}
En plus d'un stage sur le test de puces au cours duquel les étudiants sont amenées à se déplacer à Grenoble, les méthodologies de conception ASIP sont abordées. Ces méthodologies sont aux c\oe{}ur de mes travaux de thèse, et j'ai publié deux articles sur des architectures ASIP spécialisées dans le décodage de codes polaires \cite{Leonardon2018b,Leonardon2018a}. Les outils utilisés dans les TP sur les ASIP sont ceux de Tensilica avec lesquels je suis très familiers.
\`A moyen termes, nous pourrions cependant réfléchir à concevoir des travaux pratiques en utilisant des processeurs TTA (Transport Triggered Architectures) à l'aide de l'outil TCE (\url{http://openasip.org/}). L'avantage de cet outil du point de vue pédagogique est que le modèle matériel du processeur généré par l'outil est décrit en langage VHDL lisible. Il s'agit de plus d'un logiciel libre, aussi toutes les fonctionnalités sont accessibles contrairement aux outils Tensilica qui sont liés à une licence.
Les méthodologies de synthèse de haut niveau (HLS : High Level Synthesis) sont également abordées. Je n'ai pas encore d'expérience dans ce domaine mais, maîtrisant les outils de conceptions numérique et le langage C, je pense avoir toutes les compétences pour progresser rapidement.

\paragraph{UE \'Elective 2 \& 3 : Intelligence artificielle - algorithmes \& implémentation}
Je n'ai pas d'expériences particulière sur les thématiques d'intelligence artificielle et d'apprentissage profond. Toutefois, je pense que je pourrai, après m'être mis à niveau sur ces sujets, accompagner les étudiants sur l'implémentation d'algorithme sur FPGA, VHDL ou sur des implémentations logicielles.

\paragraph{UE \'Elective 4 : Systèmes embarqués et calcul parallèle}
Après avoir échangé avec le responsable de la TAF SEH, Amer Baghdadi, il apparaît que cette TAF ayant pour sujet le calcul parallèle a doit être reprise en main puisque l'ancien responsable ne pourra plus l'assurer. J'aimerais faire des propositions en ce sens.
J'identifie deux sujets importants qui ne me semblent pas abordés dans les autres UE et pourraient être accentuées dans celle-ci.
\begin{enumerate}
\item Dans les autres UE, le calcul parallèle sur processeurs généralistes n'est pas abordé. En effet, dans les processeurs modernes, il existe désormais des unités de calcul SIMD. Les processeurs embarqués dans les téléphones par exemple sont également multic\oe{}urs. Pour exploiter complètement les capacités de ces processeurs, qu'ils soient embarqués ou non, un développeur se doit de maîtriser cette programmation parallèle. \`A travers mes travaux de recherche et le projet AFF3CT, j'ai développé des compétences solides dans ce domaine : utilisation des jeux d'instructions SIMD des processeurs x86 et ARM, mise en \oe{}uvre du parallélisme multifils / multic\oe{}urs, mise en \oe{}uvre du parallélisme multin\oe{}uds.
\item Une des ambitions de la TAF est de former les ingénieurs à concevoir des solutions logicielles complexes. Pour cela, il me semble nécessaire d'aborder des compétences de génie logiciel de base, comme la programmation orientée objet, les logiciels de suivi de version ou encore des outils d'intégration continue.
\end{enumerate}

Le cours devra donc aborder : 
\begin{itemize}
    \item Architecture pipeline
    \item Parallélisme de données (SIMD)
    \item Parallélisme d'instructions (superscalaire, VLIW, architectures mulic\oe{}urs, multifils)
    \item Mémoires partagées
    \item Calcul distribué (MPI)
\end{itemize}

Les TPs se baseront sur l'utilisation du langage C++11. La librairie MIPP pourra être déployée pour l'utilisation des instructions SIMD (\url{https://github.com/aff3ct/MIPP}). La librairie standard pourra être utilisée pour la gestion de l'exécution multifils. La librairie openmpi pourra être utilisée pour effectuer des calculs distribués. Chacun de ces points sera illustré par un TP dans lequel les étudiants devront accélérer un algorithme parallélisable (multiplication de matrices, simulations Monte Carlo).
Dans ces TPs seront abordés aussi des problématiques de programmation orientée objet pour lequel le langage C++ est propice.
Le gestionnaire de version Git sera utilisé pour divulguer les codes sources du TP. Les étudiants devront l'utiliser pour gérer leur code source et proposer un rendu final.
Les étudiants devront mettre en \oe{}uvre ces techniques au cours d'un projet.

\subsubsection{TAF Conception d'Objets Communicants}

La TAF CoOC est une thématique d'approfondissement transdisciplinaire. L'étudiant devra développer des compétences techniques de conception, de gestion de projet et également aborder des problématiques éthiques en lien avec les données personnelles et de santé. Une UE c\oe{}ur est nommée \og Conception Centrée Utilisateur \fg. J'ai une expérience dans le développement d'IHM et le maquettage. Au sein de l'entreprise WorldCast Systems, où j'ai mené à bien le développement d'une IHM, de l'analyse fonctionnelle jusqu'à la programmation (lien vers le mémoire : \url{https://bit.ly/2V7fUtN}).

Un projet \og fil rouge \fg sera également proposé durant l'année aux élèves. Je pourrais également assurer le suivi de projets.
