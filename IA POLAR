L'art de de la construction des codes correcteurs d'erreurs est un domaine de recherche actif depuis la proposition de la théorie de l'information de CLaude Shannon. Diverses familles ont été proposées, de plus en plus complexes. Sur le plan conceptuel, les outils mathématiques et d'analyse sont de plus en plus raffinés. Sur le plan calculatoire, les traitements à réaliser au niveau du décodage sont de plus en plus lourds. Cette complexité du décodage a suivi la croissance exponentielle de la puissance de calcul des circuits électroniques (loi de Moore).

Aujourd'hui, la conception de ces codes correcteurs d'erreurs et des algorithmes de décodage qui leur sont associés est toujours le fruit du travail d'une communauté d'experts. Ils utilisent pour cela des outils mathématiques théoriques couplés à des expérimentations empiriques. Selon Alan Turing, les problèmes d'une telle complexité ne pourront, à terme, n'être résolu que par des machines (%TODO : affiner la pensée, citer []). Il est possible d'appréhender ce défi avec beaucoup d'angles différents. Voilà celui qui me parait le plus prometteur en l'état actuel des choses. Il correspond l'extension de l'idée présentée dans [AI Coding]. Ce principe est schématisé dans la figure BLAH.

Fig.II. / AI coding

La première étape est de partir d'une famille de code existante. Pour l'exemple, et parce qu'il s'agit des codes que je connais le mieux, je parlerai la famille des codes polaires. Des raisonnements analogues pourraient être menés pour n'importe quelle famille de codes correcteurs d'erreurs.
La construction d'un code polaire (N,K), d'une taille N et codant K bits d'informations, peut être représentée par son vecteur de bits gelés Ak, dont K bits doivent être à 1 et les N-K restants à 0. Cette construction est la donnée de sortie du "Constructeur" de la figure 1. Cette construction étant connue, il est possible de générer un encodeur et un décodeur de codes polaires, et de simuler ses performances, par exemple à l'aide d'une simulation de type Monte Carlo. Le résultat de cette évaluation (performance measure) est le taux d'erreur trame simulé.

Le principe est alors de réaliser (constructeur) un système d'apprentissage automatique qui doit améliorer les performances du code. Une telle démarche est proposée dans [].

Ce type d'approche me parait très prometteur. Il s'agit en effet de prendre un problème ayant un très grand nombre de dimensions (N, K, position des bits gelés, algorithme de décodage utilisé, ...) et de réduire ce problème à l'aide d'outil d'apprentissage machine (SVM, DNN) qui sont très performants dans ce type de traitement. Il peut y avoir des applications directes, comme dans [reinforcement] où l'apprentissage par réenforcement permet des gains substantiels en termes de performances de décodage. Il peut également y avoir des approches plus complexes, et en particulier un aller retour entre les experts qui proposent un code, et la réponse du système d'apprentissage qui proposera peut être des axes d'améliorations inattendus. Des allers-retours entre l'humain et la machine pourraient alors mener vers la définition de nouvelles familles de codes correcteurs d'erreurs.

%TODO : je tourne en rond.
Mes travaux antérieurs seront un tremplin parfait pour effectuer cette recherche. En effet le but du projet AFF3CT est la simulation très haut débit de l'ensemble des différents codes correcteurs d'erreurs, et ce de manière très générique et flexible vis-à-vis des paramètres d'entrée. Cette généricité permettra, dans le cadre de recherche représenté en figure 2, d'explorer toutes les dimensions du problèmes. En continuant sur l'exemple des codes polaires, il est par exemple possible, à l'aide d'un même code compilé, d'explorer tout couple (N,K), tout vecteur de bit gelés, tout motif de poinçonnage, et la majeure partie des algorithmes de décodage existant. Pour chaque algorithme de décodage, il est également possible de modifier des paramètres internes. Par exemple pour le décodage SCL (Successive Cancellation List), la profondeur de la liste, l'élagage de l'arbre de décodage, la quantification des données, le polynome d'un éventuel CRC concaténé, etc...
